anchor_vlm: &common_vlm
  type: vlm
  params:
    model_params: &vlm_model
      model:
        pretrained_model_name_or_path: mistral-community/pixtral-12b
        low_cpu_mem_usage: true
        trust_remote_code: true
        device_map: auto
      bitsandbytes:
        load_in_4bit: true
        bnb_4bit_quant_type: nf4
      option2label:
        A: fully_human
        B: text_generated
        C: image_generated
        D: fully_generated
    processor_params: &vlm_processor
      use_fast: true
      trust_remote_code: true
    inference_params: &vlm_inference
      prompt:
        system: |
          Your task is to determine whether this image and its caption text has been made by humans or machines.
          Specifically, chose one of these options:

          A) Both the image and text have been made by a human
          B) The text is human made, but the image was generated by an image generation model
          C) The image is human made, but the text was generated by a text generation model
          D) Both the image and text have been generated by machines
        user: |
          Text: {text}

vlm--pixtral-12-icl:
  <<: *common_vlm
  params:
    model_params:
      <<: *vlm_model
    processor_params:
      <<: *vlm_processor
    training_params:
      num_examples_per_label: 1
    inference_params:
      <<: *vlm_inference
      batch_size: 1

vlm--pixtral-12:
  <<: *common_vlm
  params:
    model_params:
      <<: *vlm_model
    processor_params:
      <<: *vlm_processor
    training_params:
      num_examples_per_label: 0
    inference_params:
      <<: *vlm_inference
      batch_size: 8

latefusion--xlm-roberta-base+clip-vit-base:
  type: late_fusion
  params:
    model_params:
      text:
        pretrained_model_name_or_path: FacebookAI/xlm-roberta-base
      image:
        pretrained_model_name_or_path: openai/clip-vit-base-patch16
      head:
        dropout: 0.1
        ffn_dim: 2048
        n_layers: 1
      label_mappings:
        label2id:
          fully_human: 0
          text_generated: 1
          image_generated: 2
          fully_generated: 3
        id2label:
          '0': fully_human
          '1': text_generated
          '2': image_generated
          '3': fully_generated
    processor_params:
      text: {}
      image:
        trust_remote_code: true
      text_encoder:
        batch_size: 16
      image_encoder:
        batch_size: 16
    training_params:
      output_dir: ./checkpoints/subtask_1/late-fusion
      per_device_train_batch_size: 16
      num_train_epochs: 5
      learning_rate: 5.0e-05
      logging_steps: 20
      save_strategy: "no"
      fp16: true
      auto_find_batch_size: true
    inference_params:
      output_dir: ./checkpoints/subtask_1/late-fusion
      per_device_train_batch_size: 16
      num_train_epochs: 5
      learning_rate: 5.0e-05
      logging_steps: 20
      save_strategy: "no"
      fp16: true
      auto_find_batch_size: true

unimodal--vit-base+me5-base:
  type: unimodal
  params:
    model_params:
      text:
        pretrained_model_name_or_path: intfloat/multilingual-e5-base
        num_labels: 2
      image:
        pretrained_model_name_or_path: google/vit-base-patch16-224-in21k
        num_labels: 2
      label_mappings:
        label2id:
          human: 0
          generated: 1
        id2label:
          '0': human
          '1': generated
    processor_params:
      text:
        model_max_length: 512
      image:
        use_fast: true
    training_params:
      text:
        output_dir: ./checkpoints/subtask_1/unimodal-multilingual-e5-base
        per_device_train_batch_size: 16
        num_train_epochs: 5
        learning_rate: 5.0e-05
        logging_steps: 20
        save_strategy: "no"
        fp16: true
        auto_find_batch_size: true
      image:
        output_dir: ./checkpoints/subtask_1/unimodal-vit-base
        per_device_train_batch_size: 16
        num_train_epochs: 5
        learning_rate: 5.0e-05
        logging_steps: 20
        save_strategy: "no"
        fp16: true
        auto_find_batch_size: true
    inference_params:
      text:
        output_dir: ./checkpoints/subtask_1/unimodal-multilingual-e5-base
        per_device_eval_batch_size: 4
        save_strategy: "no"
      image:
        output_dir: ./checkpoints/subtask_1/unimodal-vit-base
        per_device_eval_batch_size: 4
        save_strategy: "no"

functional--majority:
  type: functional
  params:
    model_params:
      strategy: most_frequent
      random_state: 0
    processor_params: {}
    training_params: {}
    inference_params: {}

uniform--random:
  type: functional
  params:
    model_params:
      strategy: uniform
      random_state: 0
    processor_params: {}
    training_params: {}
    inference_params: {}
